name: phi-3.5-mini-instruct
parameters:
  model: ./models/Phi-3.5-mini-instruct-Q6_K.gguf
  type: phi-3.5-mini-instruct
  format: gguf
backend: llama-cpp
context_size: 8192
gpu_layers: 70  # Adjust based on your GPU memory
threads: 6      # Adjust based on your CPU cores
batch_size: 64
f16: true
download_files:
  - filename: Phi-3.5-mini-instruct-Q6_K.gguf
    uri: bartowski/Phi-3.5-mini-instruct-GGUF